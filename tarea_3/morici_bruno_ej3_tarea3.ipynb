{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3915c95",
   "metadata": {},
   "source": [
    "# Tarea 3 Ejercicio 3 - Introducción a las Redes Neuronales y Deep Learning\n",
    "\n",
    "**Nombre:** Bruno Morici\n",
    "\n",
    "**ROL USM:** 202373555-8\n",
    "\n",
    "**Curso:** INF395, Introducción a las Redes Neuronales y Deep Learning\n",
    "\n",
    "**Profesor:** Alejandro Veloz\n",
    "\n",
    "**Fecha:** 9/11/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38d517",
   "metadata": {},
   "source": [
    "# Entrenar red Transformer con texto en español\n",
    "\n",
    "Este notebook extrae y documenta las funciones y clases del proyecto `gpt-trained/` (carpeta `src`) y añade celdas para preparar datos en español, entrenar el modelo y generar texto de ejemplo. Sigue las celdas en orden y ejecuta cada una."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc7c36",
   "metadata": {},
   "source": [
    "## Flujo general del ejercicio\n",
    "\n",
    "1. **Configurar el entorno**: instalar dependencias y fijar rutas a `gpt-trained/`.\n",
    "2. **Preparar datos en español**: limpiar un chat, tokenizarlo y guardar `train.pt`, `valid.pt`, `vocab.txt` y `contacts.txt`.\n",
    "3. **Definir el modelo Transformer**: copiar las clases `Head`, `MultiHeadAttention`, `Block` y `GPTLanguageModel` del proyecto base y explicarlas.\n",
    "4. **Entrenar el modelo**: reutilizar `get_batch`, `estimate_loss` y el bucle `model_training` adaptados a celdas.\n",
    "5. **Generar texto**: cargar el modelo y muestrear respuestas de ejemplo sin depender de `prompt_toolkit`.\n",
    "\n",
    "Cada sección replica el comportamiento de los scripts originales (`config.py`, `preprocess.py`, `model.py`, `train.py` y `chat.py`) para tener un flujo reproducible desde el notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc42c5",
   "metadata": {},
   "source": [
    "## 0. Instalación de dependencias\n",
    "\n",
    "Instalar las dependencias declaradas en `gpt-trained/requirements.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet -r \"gpt-trained/requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c830d003",
   "metadata": {},
   "source": [
    "## 1. Importaciones, rutas y configuración de hiperparámetros\n",
    "\n",
    "Copiamos los hiperparámetros de `config.py` y definimos rutas base para reutilizarlas en todas las celdas posteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f3cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from typing import List, Set, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Barra de progreso para el entrenamiento\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7877a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas principales\n",
    "BASE_DIR = Path(\"gpt-trained\").resolve()\n",
    "ASSETS_DIR = BASE_DIR / \"assets\"\n",
    "INPUT_CHAT_PATH = ASSETS_DIR / \"input\" / \"chat.txt\"\n",
    "OUTPUT_DIR = ASSETS_DIR / \"output\"\n",
    "MODEL_DIR = ASSETS_DIR / \"models\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e098ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametros del modelo\n",
    "block_size = 32\n",
    "embed_size = 256\n",
    "dropout = 0.2\n",
    "n_heads = 6\n",
    "n_layer = 6\n",
    "eval_iters = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Hiperparametros de entrenamiento y preprocesamiento\n",
    "learn_rate = 3e-4\n",
    "max_iters = 2000\n",
    "eval_interval = 200\n",
    "min_count_chars = 1\n",
    "min_count_tokens = 1\n",
    "end_token = \"<END>\"\n",
    "unknown_token = \"<UNK>\"\n",
    "n_chats = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b106d",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento de chats en español\n",
    "\n",
    "Replicamos `preprocess.py` y `utils.py` para limpiar conversaciones de WhatsApp en español. El flujo es:\n",
    "\n",
    "1. Eliminar caracteres muy raros.\n",
    "2. Detectar remitentes y tratarlos como tokens especiales.\n",
    "3. Tokenizar el texto respetando nombres de contacto y el token `<END>`.\n",
    "4. Reemplazar vocabulario infrecuente por `<UNK>`.\n",
    "5. Crear tensores `train.pt` y `valid.pt`, además de `vocab.txt` y `contacts.txt`.\n",
    "\n",
    "Declaramos las funciones necesarias extraídas del ejemplo entregado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99872ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_infrequent_tokens(tokens: Union[List[str], str], min_count: int) -> List[str]:\n",
    "    counts = Counter(tokens)\n",
    "    return [k for k, v in counts.items() if v <= min_count]\n",
    "\n",
    "\n",
    "def mask_tokens(tokens: List[str], mask: Set[str]) -> List[str]:\n",
    "    return [unknown_token if t in mask else t for t in tokens]\n",
    "\n",
    "\n",
    "def drop_chars(txt: str, drop: Set[str]) -> str:\n",
    "    return txt.translate(str.maketrans(\"\", \"\", \"\".join(drop)))\n",
    "\n",
    "\n",
    "def flatten_tuple(txt: List[Tuple[str, str]]) -> str:\n",
    "    return \"\".join([contact + \":\" + msg + end_token for contact, msg in txt])\n",
    "\n",
    "\n",
    "def custom_tokenizer(txt: str, spec_tokens: List[str], pattern: str = \"|\\\\d|\\\\w+|[^\\\\s]\") -> List[str]:\n",
    "    pattern = \"|\".join(spec_tokens) + pattern\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    return tokenizer.tokenize(txt)\n",
    "\n",
    "\n",
    "def get_vocab(text: Union[List[str], str]) -> List[str]:\n",
    "    return sorted(list(set(text)))\n",
    "\n",
    "\n",
    "def encode(tokens: List[str], vocab: List[str]) -> torch.Tensor:\n",
    "    rand_token = random.randint(0, len(vocab) - 1)\n",
    "    token_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "    enc = [token_to_idx.get(token, token_to_idx.get(unknown_token, rand_token)) for token in tokens]\n",
    "    return torch.tensor(enc, dtype=torch.long)\n",
    "\n",
    "\n",
    "def decode(tensor: torch.Tensor, vocab: List[str]) -> str:\n",
    "    token_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "    idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
    "    return \" \".join(idx_to_token[i.item()] for i in tensor)\n",
    "\n",
    "\n",
    "def current_time() -> str:\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "\n",
    "def print_delayed(s: str, delay: float = 0.05) -> None:\n",
    "    for char in s:\n",
    "        print(char, end=\"\", flush=True)\n",
    "        time.sleep(delay)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e538f",
   "metadata": {},
   "source": [
    "### 2.1 Función `make_train_test`\n",
    "\n",
    "Leemos `chat.txt`, tokeniza y guarda los tensores necesarios para el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f9c20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(input_path: Path = INPUT_CHAT_PATH) -> None:\n",
    "    text = input_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    infreq_chars = get_infrequent_tokens(text, min_count=min_count_chars)\n",
    "    text = drop_chars(text, set(infreq_chars))\n",
    "\n",
    "    pattern = r\"\\[(.*?)\\] (.*?): (.*)\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    text = [(contact, msg.lower()) for _, contact, msg in matches if not msg.startswith(\"\\u200e\")]\n",
    "\n",
    "    contacts = list({contact + \":\" for contact, _ in text})\n",
    "    spec_tokens = contacts + [end_token]\n",
    "\n",
    "    text_flat = flatten_tuple(text)\n",
    "    tokens = custom_tokenizer(txt=text_flat, spec_tokens=spec_tokens)\n",
    "\n",
    "    infreq_tokens = set(get_infrequent_tokens(tokens, min_count=min_count_tokens))\n",
    "    tokens = mask_tokens(tokens, infreq_tokens)\n",
    "\n",
    "    vocab = get_vocab(tokens)\n",
    "    print(f\"El corpus tiene {len(vocab)} tokens únicos.\")\n",
    "\n",
    "    data = encode(tokens, vocab)\n",
    "    n = int(0.9 * len(data))\n",
    "    train_data = data[:n]\n",
    "    valid_data = data[n:]\n",
    "\n",
    "    torch.save(train_data, OUTPUT_DIR / \"train.pt\")\n",
    "    torch.save(valid_data, OUTPUT_DIR / \"valid.pt\")\n",
    "    (OUTPUT_DIR / \"vocab.txt\").write_text(json.dumps(vocab), encoding=\"utf-8\")\n",
    "    (OUTPUT_DIR / \"contacts.txt\").write_text(json.dumps(contacts), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Datos guardados en:\")\n",
    "    print(f\"- {OUTPUT_DIR / 'train.pt'}\")\n",
    "    print(f\"- {OUTPUT_DIR / 'valid.pt'}\")\n",
    "    print(f\"- {OUTPUT_DIR / 'vocab.txt'}\")\n",
    "    print(f\"- {OUTPUT_DIR / 'contacts.txt'}\")\n",
    "    print(\"SUCCESS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a9c04c",
   "metadata": {},
   "source": [
    "### 2.2 Ejecutar el preprocesamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10ac4a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El corpus tiene 142 tokens únicos.\n",
      "Datos guardados en:\n",
      "- C:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tarea_3\\gpt-trained\\assets\\output\\train.pt\n",
      "- C:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tarea_3\\gpt-trained\\assets\\output\\valid.pt\n",
      "- C:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tarea_3\\gpt-trained\\assets\\output\\vocab.txt\n",
      "- C:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tarea_3\\gpt-trained\\assets\\output\\contacts.txt\n",
      "SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Bruno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Bruno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Bruno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tf_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Bruno\\AppData\\Local\\Temp\\ipykernel_34792\\492867216.py\", line 1, in <module>\n",
      "    make_train_test()\n",
      "  File \"C:\\Users\\Bruno\\AppData\\Local\\Temp\\ipykernel_34792\\1821440291.py\", line 23, in make_train_test\n",
      "    data = encode(tokens, vocab)\n",
      "  File \"C:\\Users\\Bruno\\AppData\\Local\\Temp\\ipykernel_34792\\714679774.py\", line 32, in encode\n",
      "    return torch.tensor(enc, dtype=torch.long)\n",
      "C:\\Users\\Bruno\\AppData\\Local\\Temp\\ipykernel_34792\\714679774.py:32: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  return torch.tensor(enc, dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_train_test()\n",
    "\n",
    "train_path = OUTPUT_DIR / \"train.pt\"\n",
    "valid_path = OUTPUT_DIR / \"valid.pt\"\n",
    "vocab_path = OUTPUT_DIR / \"vocab.txt\"\n",
    "contacts_path = OUTPUT_DIR / \"contacts.txt\"\n",
    "\n",
    "(train_path.exists(), valid_path.exists(), vocab_path.exists(), contacts_path.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31873e",
   "metadata": {},
   "source": [
    "## 3. Definición del modelo Transformer\n",
    "\n",
    "Pasos de las siguientes celdas:\n",
    "* 1. Copiar las clases `Head` y `MultiHeadAttention`, explicando su funcionamiento.\n",
    "* 2. Copiar la clase `Block`, explicando su funcionamiento.\n",
    "* 3. Copiar la clase `GPTLanguageModel`, explicando su funcionamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa920684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1)\n",
    "        wei /= math.sqrt(k.shape[-1])\n",
    "\n",
    "        tril = torch.tril(torch.ones(T, T))\n",
    "        wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.linear = nn.Linear(n_heads * head_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        heads_list = [h(x) for h in self.heads]\n",
    "        out = torch.cat(heads_list, dim=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention()\n",
    "        self.ffwd = FeedFoward()\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embedding = nn.Embedding(block_size, embed_size)\n",
    "        block_list = [Block() for _ in range(n_layer)]\n",
    "        self.blocks = nn.Sequential(*block_list)\n",
    "        self.ln_output = nn.LayerNorm(embed_size)\n",
    "        self.linear_output = nn.Linear(embed_size, vocab_size)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_output(x)\n",
    "        logits = self.linear_output(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, vocab):\n",
    "        idx_next = torch.zeros(1)\n",
    "        idx_end = encode([end_token], vocab)\n",
    "        idx_unk = encode([unknown_token], vocab)\n",
    "\n",
    "        while idx_next[0] != idx_end:\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            while idx_next[0] == idx_unk:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx[0][:-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a502704",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento del modelo\n",
    "\n",
    "En esta sección:\n",
    "* 1. Integramos las funciones de integramos las funciones de `utils.py` y `train.py`.\n",
    "* 2. Se declaran los auxiliares `get_batch` y `estimate_loss`.\n",
    "* 3. Luego el lazo `model_training` que admite un parámetro `update` para continuar entrenamientos previos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d473204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data: torch.Tensor):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model: nn.Module, data: torch.Tensor) -> float:\n",
    "    model.eval()\n",
    "    loss_list = torch.zeros(eval_iters)\n",
    "    for i in range(eval_iters):\n",
    "        X, Y = get_batch(data)\n",
    "        logits, loss = model(X, Y)\n",
    "        loss_list[i] = loss.item()\n",
    "    model.train()\n",
    "    return loss_list.mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b474ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(update: bool = False) -> GPTLanguageModel:\n",
    "    train_data = torch.load(OUTPUT_DIR / \"train.pt\")\n",
    "    valid_data = torch.load(OUTPUT_DIR / \"valid.pt\")\n",
    "    vocab = json.loads((OUTPUT_DIR / \"vocab.txt\").read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    if update:\n",
    "        try:\n",
    "            model = torch.load(MODEL_DIR / \"model.pt\")\n",
    "            print(\"Modelo existente cargado: continúa el entrenamiento.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No se encontró un modelo previo, se inicializa uno nuevo.\")\n",
    "            model = GPTLanguageModel(vocab_size=len(vocab))\n",
    "    else:\n",
    "        print(\"Entrenamiento desde cero.\")\n",
    "        model = GPTLanguageModel(vocab_size=len(vocab))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learn_rate)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parámetros a optimizar: {n_params}\")\n",
    "\n",
    "    for i in tqdm(range(max_iters)):\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "            train_loss = estimate_loss(model, train_data)\n",
    "            valid_loss = estimate_loss(model, valid_data)\n",
    "            print(f\"{current_time()} | paso {i}: train {train_loss:.4f}, valid {valid_loss:.4f}\")\n",
    "\n",
    "        x_batch, y_batch = get_batch(train_data)\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    torch.save(model, MODEL_DIR / \"model.pt\")\n",
    "    print(\"Modelo guardado en\", MODEL_DIR / \"model.pt\")\n",
    "    return model, vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8978378",
   "metadata": {},
   "source": [
    "### 4.1 Ejecutar entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f5beca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento desde cero.\n",
      "Parámetros a optimizar: 4790926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:15:48 | paso 0: train 5.0486, valid 5.0877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 200/2000 [02:59<15:28,  1.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:18:31 | paso 200: train 0.3215, valid 4.3804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [05:09<11:36,  2.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:20:38 | paso 400: train 0.1653, valid 4.9521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 600/2000 [06:53<10:19,  2.26it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:22:22 | paso 600: train 0.1380, valid 5.2489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 800/2000 [08:30<08:20,  2.40it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:23:58 | paso 800: train 0.1319, valid 5.6288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1000/2000 [10:06<06:50,  2.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:25:35 | paso 1000: train 0.1269, valid 5.7234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1200/2000 [11:45<05:19,  2.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:27:13 | paso 1200: train 0.1251, valid 5.7921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1400/2000 [13:20<04:11,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:28:48 | paso 1400: train 0.1220, valid 5.9352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1600/2000 [14:55<02:48,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:30:23 | paso 1600: train 0.1157, valid 6.0960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1800/2000 [16:30<01:23,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:32:00 | paso 1800: train 0.1170, valid 6.1779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1999/2000 [18:07<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:33:35 | paso 1999: train 0.1137, valid 6.2429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [18:19<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en C:\\Users\\Bruno\\Desktop\\INF395 - IRN\\tareas\\tarea_3\\gpt-trained\\assets\\models\\model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, vocab = model_training(update=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2844c",
   "metadata": {},
   "source": [
    "## 5. Generación de texto en español\n",
    "\n",
    "Para simplificar la interacción en notebook, cargamos el modelo guardado y generamos respuestas dadas unas pocas frases de contexto. La lógica de muestreo replica la función `conversation()` de `chat.py` sin depender de la terminal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "491211d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_assets():\n",
    "    vocab = json.loads((OUTPUT_DIR / \"vocab.txt\").read_text(encoding=\"utf-8\"))\n",
    "    contacts = json.loads((OUTPUT_DIR / \"contacts.txt\").read_text(encoding=\"utf-8\"))\n",
    "    model = torch.load(MODEL_DIR / \"model.pt\")\n",
    "    model.eval()\n",
    "    return model, vocab, contacts\n",
    "\n",
    "\n",
    "def generate_response(seed_contact: str, mensaje: str, n_samples: int = 1):\n",
    "    model, vocab, contacts = load_model_and_assets()\n",
    "    spec_tokens = contacts + [end_token]\n",
    "\n",
    "    prompt = f\"{seed_contact}:{mensaje}{end_token}\"\n",
    "    tokens = custom_tokenizer(prompt, spec_tokens)\n",
    "    context = encode(tokens, vocab).unsqueeze(0)\n",
    "    prompt_len = context.shape[1]\n",
    "\n",
    "    respuestas = []\n",
    "    for _ in range(n_samples):\n",
    "        output = model.generate(context.clone(), vocab)\n",
    "        continuation = output[prompt_len:]\n",
    "        texto = decode(continuation, vocab)\n",
    "        texto = texto.replace(end_token, \"\").strip()\n",
    "        respuestas.append(texto)\n",
    "\n",
    "    return respuestas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af5738",
   "metadata": {},
   "source": [
    "### 5.1 Ejemplo rápido\n",
    "\n",
    "Creamos un mensaje inicial. El modelo generará `n_samples` respuestas en español basadas en el contexto entrenado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce939bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta 1: Brokovski: esperar para mí , pero va bien .\n",
      "\n",
      "Respuesta 2: Brokovski: yo también estoy pensando en explorar la palomitas , ¡ no puedo unas vacaciones en la playa .\n",
      "\n",
      "Respuesta 3: Brokovski: estoy planeando un viaje a europa el semana una gran una lista de las películas de superhéroes .\n",
      "\n",
      "Respuesta 4: Sandra: estoy pensando en explorar la europa , el caribe , bali y un viaje por carretera ¡ todos un viaje por carretera ¡ todos carretera ¡ todos todos , sandra !\n",
      "\n",
      "Respuesta 5: Brokovski: estoy planeando un viaje a europa el día .\n",
      "\n",
      "Respuesta 6: Brokovski: .\n",
      "\n",
      "Respuesta 7: Brokovski: estoy planeando un viaje a europa el es un viaje a europa el vino !\n",
      "\n",
      "Respuesta 8: Brokovski: estoy planeando un viaje a europa el una , ¡ europa suena increíble ! ¿ a\n",
      "\n",
      "Respuesta 9: Sandra: estoy pensando en explorar la , tom . ¿ de qué a canciones ?\n",
      "\n",
      "Respuesta 10: Brokovski: europa , el caribe , bali y un viaje por carretera ¡ todos .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed_contact = \"contacto:\"\n",
    "mensaje = \"¿Cómo va todo hoy?\"\n",
    "respuestas = generate_response(seed_contact, mensaje, n_samples=10)\n",
    "for idx, texto in enumerate(respuestas, start=1):\n",
    "    print(f\"Respuesta {idx}: {texto}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9b069",
   "metadata": {},
   "source": [
    "## 6. Resumen\n",
    "\n",
    "- Encapsulamos los scripts del ejemplo entregado en clases `gpt-trained/` sin modificar la arquitectura del Transformer.\n",
    "- Seccionamos las tareas en celdas para facilitar la ejecución paso a paso.\n",
    "- Documentamos en proceso completo; Tokenización, preprocesamiento, definición del modelo, entrenamiento y generación de texto..\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
